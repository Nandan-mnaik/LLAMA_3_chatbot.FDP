{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLAMA 3 CHATbot using APIs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall see how to use Llama 3 APIs to create a simple chatbot like ChatGPT for basic day to day use cases.\n",
    "We will be using Replicate API tokens to converse with the model and Streamlit to host the platform and make it into a working standard web app. \n",
    "\n",
    "LLaMA 3 (Large Language Model Meta AI) is the third version of Meta's language models, designed to handle complex natural language processing tasks such as text generation, summarization, question answering, and chatbot development. Creating a chatbot using LLaMA 3 APIs involves leveraging the power of this sophisticated model to provide meaningful and conversational responses. Here’s how to understand the concept and process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How We shall go about this app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Get a Replicate API token\n",
    "\n",
    " - Set up the coding environment\n",
    " \n",
    " - Build the app\n",
    " \n",
    " - Set the API token\n",
    " \n",
    " - Deploy the app (If we want to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Overview:\n",
    "Here is a high-level overview of the Llama3.1 chatbot app:\n",
    "\n",
    "The user provides two inputs: \n",
    " - A Replicate API token (if requested) and \n",
    "\n",
    " - A prompt input (i.e. ask a question).\n",
    "An API call is made to the Replicate server, where the prompt input is submitted and the resulting LLM-generated response is obtained and displayed in the app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Requirements/Installations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up this basic Llama based chatbot, the following libraries and tools are required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.**Python**: Ensure Python (version 3.8 or higher) is installed in your systems.\n",
    "\n",
    "2.**Streamlit**: Streamlit is an open-source Python framework that simplifies the process of building and sharing web applications, especially for data science and machine learning tasks.\n",
    "\n",
    "3.**Replicate**: Replicate is a platform that provides developers and researchers with an easy way to run machine learning models in the cloud and deploy them as APIs without needing to manage infrastructure or deal with complex deployment workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install Replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Procedure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Get a Replicate API token**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting your Replicate API token is a simple 3-step process:\n",
    "\n",
    "Go to https://replicate.com/signin/.\n",
    "\n",
    "Sign in with your GitHub account.\n",
    "\n",
    "Proceed to the API tokens page and copy your API token. once this step is over we move to the real coding part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Import necessary libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import replicate\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Define the app title**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The title of the app displayed on the browser can be specified using the page_title parameter, which is defined in the st.set_page_config() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.set_page_config(page_title=\"AI Chatbot using LLama 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Defining a sidebar to accept the API token and adjust model parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing the chatbot app, divide the app elements by placing the app title and text input box for accepting the Replicate API token in the sidebar and the chat input text in the main panel. To do this, place all subsequent statements under with st.sidebar:, followed by the following steps:\n",
    "\n",
    "1. Define the app title using the st.title() method.\n",
    "\n",
    "2. Use if-else statements to conditionally display either:\n",
    "\n",
    " - A success message in a green box that reads API key already provided! for the if statement.\n",
    "\n",
    " - A warning message in a yellow box along with a text input box asking for the API token, as none were detected in the Secrets, for the else statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with st.sidebar:\n",
    "    st.title('Llama 3.1 Chatbot')\n",
    "    # Enter Replicate API token\n",
    "    replicate_api = st.text_input('Enter Replicate API token:', type='password')\n",
    "    # Model parameters\n",
    "    temperature = st.slider('Temperature', min_value=0.01, max_value=1.0, value=0.5, step=0.01)\n",
    "    top_p = st.slider('Top P', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
    "    max_length = st.slider('Max Length', min_value=32, max_value=512, value=128, step=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the above code we will also be making toggles to set up the parameters such as temperature (creativity) , top_p (nucleus sampling) and max word length of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Setting up Replicate API token**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Inside the function, there’s a simple check: if replicate_api. This checks whether the replicate_api value is provided. If it is None or an empty string, the block of code will not run.\n",
    "\n",
    "If a valid API token is passed, the line \n",
    "\n",
    "os.environ['REPLICATE_API_TOKEN'] = replicate_api \n",
    "\n",
    "sets the environment variable REPLICATE_API_TOKEN to the value of replicate_api. This is important because many libraries or services (like Replicate) use environment variables to securely store sensitive information like API keys. By storing the token in an environment variable, you avoid hardcoding it into the script, which is a best practice for security reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replicate_api:\n",
    "    os.environ['REPLICATE_API_TOKEN'] = replicate_api\n",
    "\n",
    "# Define the Replicate Llama 3.1 model version\n",
    "llama_3_1_model = \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_3_1_model = \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the API that we are using to create this Chatbot. This API was taken from Replicate you can also get similar APIs from other open-source platforms such as HuggingFace etc...\n",
    "\n",
    "In case we want to use OpenAI API keys, we need to pay for it and the key will be linked to your OpenAI account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **Chat History Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making a Generative AI chatbot, it is essential to have good UI to talk to the chatbot. While doing this we need to enable history inorder for the model to learn your prefernces \"for RAG\"\n",
    "\n",
    "We have 3 Self defining functions here that is used for chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chat_history():\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_chat():\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.write(message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_chat_history():\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
    "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. **Response generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a custom function in order to get our response from the Llama API.\n",
    "\n",
    "This function takes the user prompt as input and builds a connection to the replicate servers from where the model is called upon and the prompt is decoded and an output response is generated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt_input):\n",
    "    if not replicate_api:\n",
    "        st.warning(\"Please enter a valid Replicate API token.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # API request to Replicate\n",
    "        output = replicate.run(\n",
    "            llama_3_1_model,\n",
    "            input={\n",
    "                \"prompt\": prompt_input,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"max_length\": max_length,\n",
    "                \"repetition_penalty\": 1.0\n",
    "            }\n",
    "        )\n",
    "        # Join the output response if it's a list (Replicate API often returns a list)\n",
    "        return ''.join(output) if isinstance(output, list) else output\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error calling Replicate API: {e}\")\n",
    "        return None\n",
    "\n",
    "# User input for chat\n",
    "prompt = st.chat_input(\"Enter your message here...\")\n",
    "\n",
    "# Generate and display response if a prompt is entered\n",
    "if prompt and replicate_api:\n",
    "    # Add user input to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(prompt)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            response = generate_response(prompt)\n",
    "            if response:\n",
    "                st.write(response)\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            else:\n",
    "                st.warning(\"Failed to generate a response. Please try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
